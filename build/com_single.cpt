// File generated by hilapp at Thu Mar 10 09:10:20 2022
// Git version: e3ba0735
// cmd: ../../HILA/hila/hilapp/bin/hilapp -I/opt/shared/openmpi/1.8.2/include 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/include/c++/9.3.0 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/lib/gcc/x86_64-pc-linux-gnu/9.3.0/../../../../include/c++/9.3.0/x86_64-pc-linux-gnu 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/lib/gcc/x86_64-pc-linux-gnu/9.3.0/../../../../include/c++/9.3.0/backward 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/lib/gcc/x86_64-pc-linux-gnu/9.3.0/include 
//        -I/usr/local/include -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/include 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/lib/gcc/x86_64-pc-linux-gnu/9.3.0/include-fixed 
//        -I/usr/include -I/opt/shared/openmpi/1.8.2/include -I/opt/shared/fftw/3.3.2/include/ 
//        -DUSE_MPI -I../../HILA/hila/hilapp/clang_include -DNDIM=3 
//        -DUSE_MPI -I/opt/shared/fftw/3.3.2/include/ -I../../HILA/hila/libraries 
//        -I../../HILA/hila/libraries/plumbing -DGIT_SHA_VALUE=b1aa3587 
//        ../../HILA/hila/libraries/plumbing/com_single.cpp -o build/com_single.cpt 
//        


#ifndef USE_MPI
// protect with USE_MPI so can be included even in no-MPI codes

#include "plumbing/defs.h"
#include "plumbing/lattice.h"
#include "plumbing/field.h"

static bool initialized = false;

/* Machine initialization */
#include <sys/types.h>
void initialize_machine(int &argc, char ***argv) {
    /* Init MPI */
    if (!initialized) {
        lattice->mynode.rank = 0;
        lattice->nodes.number = 1;

#if defined(CUDA) || defined(HIP)
        initialize_cuda(0);
#endif
        initialized = true;
    }
    hila::my_rank_n = 0;
}

// check if is intialized
bool is_comm_initialized(void) { return initialized; }

/* version of exit for multinode processes -- kill all nodes */
void abort_communications(int status) {}

/* clean exit from all nodes */
void finish_communications() {}

/* BASIC COMMUNICATIONS FUNCTIONS */

/* Tell what kind of machine we are on */
char *machine_type() {
    static char name[] = "SINGLE (no MPI)";
    return name;
}

/* Return my node number */
int hila::myrank() { return 0; }

/* Return number of nodes */
int hila::number_of_nodes() { return 1; }

void hila::synchronize() { hila::synchronize_threads(); }

// Split the communicator to subvolumes, using MPI_Comm_split
// New MPI_Comm is the global mpi_comm_lat
// NOTE: no attempt made here to reorder the nodes

void split_into_sublattices(int this_lattice) {}

#if 0

// Switch comm frame global-sublat
// for use in io_status_file

void reset_comm(bool global)
{
  static MPI_Comm mpi_comm_saved;
  static int set = 0;

  g_sync_sublattices();
  if (global) {
    mpi_comm_saved = lattice->mpi_comm_lat;
    lattice->mpi_comm_lat = MPI_COMM_WORLD;
    set = 1;
  } else {
    if (!set) halt("Comm set error!");
    lattice->mpi_comm_lat = mpi_comm_saved;
    set = 0;
  }
  mynode = hila::myrank();
}

#endif

#endif // USE_MPI
