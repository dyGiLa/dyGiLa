// File generated by hilapp at Thu Mar 10 09:09:58 2022
// Git version: e3ba0735
// cmd: ../../HILA/hila/hilapp/bin/hilapp -I/opt/shared/openmpi/1.8.2/include 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/include/c++/9.3.0 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/lib/gcc/x86_64-pc-linux-gnu/9.3.0/../../../../include/c++/9.3.0/x86_64-pc-linux-gnu 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/lib/gcc/x86_64-pc-linux-gnu/9.3.0/../../../../include/c++/9.3.0/backward 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/lib/gcc/x86_64-pc-linux-gnu/9.3.0/include 
//        -I/usr/local/include -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/include 
//        -I/cluster/tufts/hpc/tools/spack/linux-rhel7-sandybridge/gcc-8.4.0/gcc-9.3.0-hv7dnzblbdfkadid4q76jcdpq5sfyfkb/lib/gcc/x86_64-pc-linux-gnu/9.3.0/include-fixed 
//        -I/usr/include -I/opt/shared/openmpi/1.8.2/include -I/opt/shared/fftw/3.3.2/include/ 
//        -DUSE_MPI -I../../HILA/hila/hilapp/clang_include -DNDIM=3 
//        -DUSE_MPI -I/opt/shared/fftw/3.3.2/include/ -I../../HILA/hila/libraries 
//        -I../../HILA/hila/libraries/plumbing -DGIT_SHA_VALUE=b1aa3587 
//        ../../HILA/hila/libraries/plumbing/random.cpp -o build/random.cpt 
//        

#include <cmath>
// start include "hila.h"---------------------------------
#ifndef HILA_H_
#define HILA_H_

///////////////////////////////////////////////////////////////////////
/// Catch-(almost)all include to get in most of the hila-system .h -files


// start include "plumbing/defs.h"---------------------------------
#ifndef DEFS_H_
#define DEFS_H_

// This gives us math constants, e.g. M_PI etc.
#define _USE_MATH_DEFINES

// Useful global definitions here -- this file should be included by (almost) all others

#include <iostream>
#include <array>
#include <vector>
#include <assert.h>
#include <sstream>
// #include <math.h>
#include <type_traits>
#include <cmath>


#ifdef USE_MPI
#ifdef HILAPP
// Removed by hilapp"/cluster/home/alopez07/HILA/hila/libraries/plumbing/hilapp_mpi.h"
#else
#include <mpi.h>
#endif
#endif

// Read in Makefile tunable parameters first
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/params.h"

#ifdef HILAPP
// Removed by hilapp
#endif

#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/mersenne.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/memalloc.h" // memory allocator
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/timing.h"


/// Define __restrict__?  It is non-standard but supported by most (all?) compilers.
/// ADD HERE GUARD FOR THOSE WHICH DO not HAVE IT
#define RESTRICT __restrict__
// #ifndef CUDA
// #define RESTRICT __restrict__
// #else
// #define RESTRICT // disabled here
// #endif


// This below declares "out_only" -qualifier.  It is empty on purpose. Do not remove!
#define out_only
// out_only for methods tells hilapp that the base variable original value is not
// needed: class C {
//    int set() out_only { .. }
// };
// indicates that a.set(); does not need original value of a.
// Can also be used in function arguments:
//    int func( out_only double & p, ..);

// Defined empty on purpose, same as above!
#define const_function
// const_function does not change the base variable, but can return a (non-const)
// reference. Needed typically for access operators for loop extern variables:
//     class v {
//         double c[N];
//         double & e(const int i) const_function { return c[i]; }
//     };
//
//     v vv;
//     Field<v>  f;
//     onsites(ALL) { f[X].e(0) += vv.e(0); }
// This would not work without const_function, because vv.e(1) might modify loop
// extern variable vv, which is not allowed.  If method is marked "const",
// then the assignment would not work.
//
// const_function is weaker than const.


// text output section -- defines also output0, which writes from node 0 only
namespace hila {

/// this is our default output file stream
extern std::ostream output;
/// this is just a hook to store output file, if it is in use
extern std::ofstream output_file;

// about_to_finish becomes true at the end.  Signals that
// better not rely on MPI or existence of objects any more.
extern bool about_to_finish;

// check_input is used to notify that we're just checking the
// input values and will exit before fields are allocated.
extern bool check_input;
extern int check_with_nodes;

// optional input filename
extern const char *input_file;

void initialize(int argc, char **argv);
void finishrun();
void terminate(int status);
void error(const std::string &msg);
void error(const char *msg);


/// rank of this node
int myrank();
/// how many nodes there are
int number_of_nodes();
/// synchronize mpi
void synchronize();


} // namespace hila

// The logger uses hila::myrank, so it cannot be included on top
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/logger.h"
namespace hila {
/// Now declare the logger
extern logger_class log;
} // namespace hila

/// We want to define ostream
///     "output0 << stuff;"
/// which is done only by rank 0.
/// This is hacky but easy.  Probably should be done without #define.
/// Do this through else-branch in order to avoid if-statement problems.
/// #define output0 if (hila::myrank() != 0) {} else hila::output
///
/// Above #define can trigger "dangling-else" warning.  Let us
/// try to avoid it with the following a bit more ugly trick:
#define output0                                                                        \
    for (int _dummy_i_ = 1; hila::myrank() == 0 && _dummy_i_; --_dummy_i_)             \
    hila::output

// The following disables the "dangling-else" warning, but not needed now
//#if defined(__clang__) || defined(__GNUC__)
//#pragma GCC diagnostic ignored "-Wdangling-else"
//#endif

/// define a class for FFT direction
enum class fft_direction { forward, back };

// trivial template for helping vectorization
template <typename T>
using element = T;

// Backend defs-headers

#if defined(CUDA) || defined(HIP)
#include "plumbing/backend_cuda/defs.h"
#elif defined(AVX)
#include "plumbing/backend_vector/defs.h"
#else
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/backend_cpu/defs.h"
#endif

// this include has to be after the backend defs, because those define hila::random()
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/random.h"

// This contains useful template tools
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/type_tools.h"

#if defined(CUDA) || defined(HIP)
#include "plumbing/backend_cuda/cuda_templated_ops.h"
#endif

// Include some basic functions for real (non-class) vars,
// to help with generic code
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/real_var_ops.h"

// MPI Related functions and definitions
#define MAX_GATHERS 1000

#ifndef USE_MPI

// broadcast does nothing if not MPI
template <typename T>
void broadcast(T &v) {}

template <typename T>
void broadcast_array(T *var, int n) {}

#endif

void initialize_communications(int &argc, char ***argv);
void split_into_sublattices(int rank);
bool is_comm_initialized(void);
void finish_communications();
void abort_communications(int status);

// and print a dashed line
void print_dashed_line(const std::string &txt = {});


#endif
// end include "plumbing/defs.h"---------------------------------

#include "/cluster/home/alopez07/HILA/hila/libraries/datatypes/cmplx.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/datatypes/matrix.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/datatypes/sun_matrix.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/datatypes/u1.h"

#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/coordinates.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/lattice.h"
// start include "plumbing/field.h"---------------------------------
#ifndef FIELD_H
#define FIELD_H
#include <sstream>
#include <iostream>
#include <string>
#include <cstring> //Memcpy is here...
#include <math.h>
#include <type_traits>

#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/defs.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/coordinates.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/lattice.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/field_storage.h"

#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/backend_vector/vector_types.h"

#ifdef USE_MPI
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/com_mpi.h"
#endif

// This is a marker for hilapp -- will be removed by it
#define onsites(p) for (Parity par_dummy__(p); par_dummy__ == EVEN; par_dummy__ = ODD)

template <typename T>
class Field;

template <typename T>
void ensure_field_operators_exist(Field<T> &f);


/// Field class
/// This implements the standard methods for accessing fields
/// Hilapp replaces the parity access patterns, Field[par] with a loop over
/// the appropriate sites.
///
/// The Field class also contains member functions used by hilapp, as well
/// as members that may be useful for application developers.
///
/// The Field mainly implements the interface to the Field and not the
/// content.
///
/// The Field contains a pointer to Field::field_struct, which implements
/// MPI communication of the Field boundaries.
///
/// The Field::field_struct points to a field_storage, which is defined
/// by each backend. It implements storing and accessing the Field data,
/// including buffers for storing haloes returned from MPI communication.
///
/// Memory allocation (mainly automised by hilapp):
/// Field.allocate(): sets up memory for field content and communication.
/// Field.free(): destroys the data.
/// Field.is_allocated(): returns true if the Field data has been allocated
/// Field.is_initialized() returns true if the Field has been written
/// Field.check_alloc(): allocate if necessary
/// Field.check_alloc() const: assert that the Field is allocated
///
/// MPI related (automatically done by hilapp, but may be useful in apps):
/// Field.move_status(): returns current fetch_status
/// Field.mark_changed(): make sure the Field gets communicated
/// Field.mark_fetched(): mark the Field already fetched, no need to
///        communicate.
///
/// Field.shift(): create a periodically shifted copy of the field
///
/// Others
/// Field.set_boundary_condition(): set the boundary conditions in a
///         given Direction (periodic or antiperiodic)
/// Field.get_boundary_condition(): get the boundary condition of the Field
/// Field.copy_boundary_condition(): copy the boundary condition to the
///        from another Field
/// Field.get_elements(): retrieve a list of elements to all nodes
/// Field.get_element(): retrieve an element to all nodes
/// Field.set_elements(): set elements in the Field
/// Field.set_element(): set an element in the Field
///
template <typename T>
class Field {

  public:
    enum class fetch_status : unsigned { NOT_DONE, STARTED, DONE };

  private:
    /// The following struct holds the data + information about the field
    /// TODO: field-specific boundary conditions?
    class field_struct {
      public:
        field_storage<T>
            payload; // TODO: must be maximally aligned, modifiers - never null
        lattice_struct *lattice;
#ifdef VECTORIZED
        // get a direct ptr from here too, ease access
        vectorized_lattice_struct<hila::vector_info<T>::vector_size> *vector_lattice;
#endif
        unsigned assigned_to; // keeps track of first assignment to parities
        fetch_status move_status[3][NDIRS]; // is communication done

        // neighbour pointers - because of boundary conditions, can be different for
        // diff. fields
        const unsigned *RESTRICT neighbours[NDIRS];
        BoundaryCondition boundary_condition[NDIRS];

#ifdef USE_MPI
        MPI_Request receive_request[3][NDIRS];
        MPI_Request send_request[3][NDIRS];
#ifndef VANILLA
        // vanilla needs no special receive buffers
        T *receive_buffer[NDIRS];
#endif
        T *send_buffer[NDIRS];

        void initialize_communication() {
            for (int d = 0; d < NDIRS; d++) {
                for (int p = 0; p < 3; p++)
                    move_status[p][d] = fetch_status::NOT_DONE;
                send_buffer[d] = nullptr;
#ifndef VANILLA
                receive_buffer[d] = nullptr;
#endif
            }
        }

        void free_communication() {
            for (int d = 0; d < NDIRS; d++) {
                if (send_buffer[d] != nullptr)
                    payload.free_mpi_buffer(send_buffer[d]);
#ifndef VANILLA
                if (receive_buffer[d] != nullptr)
                    payload.free_mpi_buffer(receive_buffer[d]);
#endif
            }
        }

#else // Not MPI

        // empty stubs
        void initialize_communication() {}
        void free_communication() {}

#endif

        void allocate_payload() {
            payload.allocate_field(lattice);
        }
        void free_payload() {
            payload.free_field();
        }

#ifndef VECTORIZED
        /// Getter for an individual elements in a loop
        inline auto get(const unsigned i) const {
            return payload.get(i, lattice->field_alloc_size());
        }

        template <typename A>
        inline void set(const A &value, const unsigned i) {
            payload.set(value, i, lattice->field_alloc_size());
        }

        /// Getter for an element outside a loop. Used to manipulate the field directly
        /// outside loops.
        inline auto get_element(const unsigned i) const {
            return payload.get_element(i, lattice);
        }

        template <typename A>
        inline void set_element(const A &value, const unsigned i) {
            payload.set_element(value, i, lattice);
        }
#else
        template <typename vecT>
        inline vecT get_vector(const unsigned i) const {
            return payload.template get_vector<vecT>(i);
        }
        inline T get_element(const unsigned i) const {
            return payload.get_element(i);
        }

        template <typename vecT>
        inline void set_vector(const vecT &val, const unsigned i) {
            return payload.set_vector(val, i);
        }
        inline void set_element(const T &val, const unsigned i) {
            return payload.set_element(val, i);
        }
#endif

        /// Gather boundary elements for communication
        void
        gather_comm_elements(Direction d, Parity par, T *RESTRICT buffer,
                             const lattice_struct::comm_node_struct &to_node) const {
#ifndef VECTORIZED
#ifdef SPECIAL_BOUNDARY_CONDITIONS
            // note: -d in is_on_edge, because we're about to send stuff to that
            // Direction (fetching from Direction +d)
            if (boundary_condition[d] == BoundaryCondition::ANTIPERIODIC &&
                lattice->special_boundaries[-d].is_on_edge) {
                payload.gather_comm_elements(buffer, to_node, par, lattice, true);
            } else {
                payload.gather_comm_elements(buffer, to_node, par, lattice, false);
            }
#else
            payload.gather_comm_elements(buffer, to_node, par, lattice, false);
#endif

#else
            // this is vectorized branch
            bool antiperiodic = false;
#ifdef SPECIAL_BOUNDARY_CONDITIONS
            if (boundary_condition[d] == BoundaryCondition::ANTIPERIODIC &&
                lattice->special_boundaries[-d].is_on_edge) {
                antiperiodic = true;
            }
#endif

            if constexpr (hila::is_vectorizable_type<T>::value) {
                // now vectorized layout
                if (vector_lattice->is_boundary_permutation[abs(d)]) {
                    // with boundary permutation need to fetch elems 1-by-1
                    int n;
                    const unsigned *index_list = to_node.get_sitelist(par, n);
                    if (!antiperiodic) {
                        payload.gather_elements(buffer, index_list, n, lattice);
                    } else {
                        payload.gather_elements_negated(buffer, index_list, n, lattice);
                    }
                } else {
                    // without it, can do the full block
                    payload.gather_comm_vectors(buffer, to_node, par, vector_lattice,
                                                antiperiodic);
                }
            } else {
                // not vectoizable, standard methods
                int n;
                const unsigned *index_list = to_node.get_sitelist(par, n);
                if (!antiperiodic)
                    payload.gather_elements(buffer, index_list, n, lattice);
                else {
                    payload.gather_elements_negated(buffer, index_list, n, lattice);
                }
            }
#endif
        }

        /// Place boundary elements from neighbour
        void place_comm_elements(Direction d, Parity par, T *RESTRICT buffer,
                                 const lattice_struct::comm_node_struct &from_node) {
// #ifdef USE_MPI
#ifdef VECTORIZED
            if constexpr (hila::is_vectorizable_type<T>::value) {
                // now vectorized layout, act accordingly
                if (vector_lattice->is_boundary_permutation[abs(d)]) {
                    payload.place_recv_elements(buffer, d, par, vector_lattice);
                } else {
                    // nothing to do here, comms directly in place
                }
            } else {
                // non-vectorized, using vanilla method, again nothing to do
            }
#else
            // this one is only for CUDA
            payload.place_comm_elements(d, par, buffer, from_node, lattice);
#endif
            // #endif
        }

        /// Place boundary elements from local lattice (used in vectorized version)
        void set_local_boundary_elements(Direction dir, Parity par) {

#ifdef SPECIAL_BOUNDARY_CONDITIONS
            bool antiperiodic =
                (boundary_condition[dir] == BoundaryCondition::ANTIPERIODIC &&
                 lattice->special_boundaries[dir].is_on_edge);
#else
            bool antiperiodic = false;
#endif
            payload.set_local_boundary_elements(dir, par, lattice, antiperiodic);
        }

        /// Gather a list of elements to a single node
        void gather_elements(T *buffer, std::vector<CoordinateVector> coord_list,
                             int root = 0) const;
        void send_elements(T *buffer, std::vector<CoordinateVector> coord_list,
                           int root = 0);

#if defined(USE_MPI)

        /// get the receive buffer pointer for the communication.
        T *get_receive_buffer(Direction d, Parity par,
                              const lattice_struct::comm_node_struct &from_node) {
#if defined(VANILLA)

            return (T *)payload.get_buffer() + from_node.offset(par);

#elif defined(CUDA) || defined(HIP)

            unsigned offs = 0;
            if (par == ODD)
                offs = from_node.sites / 2;
            if (receive_buffer[d] == nullptr) {
                receive_buffer[d] = payload.allocate_mpi_buffer(from_node.sites);
            }
            return receive_buffer[d] + offs;

#elif defined(VECTORIZED)

            if constexpr (!hila::is_vectorizable_type<T>::value) {
                // use vanilla type, field laid out in std fashion
                return (T *)payload.get_buffer() + from_node.offset(par);
            } else {
                unsigned offs = 0;
                if (par == ODD)
                    offs = from_node.sites / 2;

                if (vector_lattice->is_boundary_permutation[abs(d)]) {
                    // extra copy operation needed
                    if (receive_buffer[d] == nullptr) {
                        receive_buffer[d] =
                            payload.allocate_mpi_buffer(from_node.sites);
                    }
                    return receive_buffer[d] + offs;
                } else {
                    // directly to halo buffer
                    constexpr unsigned vector_size = hila::vector_info<T>::vector_size;
                    return ((T *)payload.get_buffer() +
                            (vector_lattice->halo_offset[d] * vector_size + offs));
                }
            }
#endif
        } // end of get_receive_buffer
#endif    // USE_MPI
    };

    // static_assert( std::is_pod<T>::value, "Field expects only pod-type elements
    // (plain data): default constructor, copy and delete");
    static_assert(std::is_trivial<T>::value && std::is_standard_layout<T>::value,
                  "Field expects only pod-type elements (plain data): default "
                  "constructor, copy and delete");

  public:
    ////////////////////////////////////////////////
    /// Field::fs keeps all of the field content
    ////////////////////////////////////////////////

    field_struct *RESTRICT fs;

    ////////////////////////////////////////////////
    /// Field constructors

    Field() {
        fs = nullptr; // lazy allocation on 1st use
    }

    // Straightforward copy constructor seems to be necessary
    Field(const Field &other) {
        fs = nullptr; // this is probably unnecessary
        if (other.fs != nullptr) {
            (*this)[ALL] = other[X];
        }
    }

    // copy constructor - from fields which can be assigned
    template <typename A, std::enable_if_t<std::is_convertible<A, T>::value, int> = 0>
    Field(const Field<A> &other) {
        fs = nullptr; // this is probably unnecessary
        if (other.fs != nullptr) {
            (*this)[ALL] = other[X];
        }
    }

    // constructor with compatible scalar
    template <typename A, std::enable_if_t<hila::is_assignable<T &, A>::value ||
                                               std::is_convertible<A, T>::value,
                                           int> = 0>
    Field(const A &val) {
        fs = nullptr;
        (*this)[ALL] = val;
    }

    // constructor from 0 - nullptr trick in use
    Field(const std::nullptr_t z) {
        fs = nullptr;
        (*this)[ALL] = 0;
    }

    // move constructor - steal the content
    Field(Field &&rhs) {
        // std::cout << "in move constructor\n";
        fs = rhs.fs;
        rhs.fs = nullptr;
    }

    /////////////////////////////////////////////////
    /// Destructor

    ~Field() {
        free();

#ifdef HILAPP
// Removed by hilapp
#endif
    }

    void allocate() {
        assert(fs == nullptr);
        if (lattice == nullptr) {
            output0 << "Can not allocate Field variables before lattice.setup()\n";
            hila::terminate(0);
        }
        fs = (field_struct *)memalloc(sizeof(field_struct));
        fs->lattice = lattice;
        fs->allocate_payload();
        fs->initialize_communication();
        mark_changed(ALL);   // guarantees communications will be done
        fs->assigned_to = 0; // and this means that it is not assigned

        for (Direction d = (Direction)0; d < NDIRS; ++d) {

#if !defined(CUDA) && !defined(HIP)
            fs->neighbours[d] = lattice->neighb[d];
#else
            fs->payload.neighbours[d] = lattice->backend_lattice->d_neighb[d];
#endif
        }

#ifdef SPECIAL_BOUNDARY_CONDITIONS
        foralldir (dir) {
            fs->boundary_condition[dir] = BoundaryCondition::PERIODIC;
            fs->boundary_condition[-dir] = BoundaryCondition::PERIODIC;
        }
#endif

#ifdef VECTORIZED
        fs->vector_lattice =
            lattice->backend_lattice
                ->get_vectorized_lattice<hila::vector_info<T>::vector_size>();
#endif
    }

    void free() {
        // don't call destructors when exiting - either MPI or cuda can already
        // be off.
        if (fs != nullptr && !hila::about_to_finish) {
            for (Direction d = (Direction)0; d < NDIRS; ++d)
                drop_comms(d, ALL);
            fs->free_payload();
            fs->free_communication();
            std::free(fs);
            fs = nullptr;
        }
    }

    bool is_allocated() const {
        return (fs != nullptr);
    }

    bool is_initialized(Parity p) const {
        return fs != nullptr && ((fs->assigned_to & parity_bits(p)) != 0);
    }

    fetch_status move_status(Parity p, int d) const {
        assert(parity_bits(p) && d >= 0 && d < NDIRS);
        return fs->move_status[(int)p - 1][d];
    }
    void set_move_status(Parity p, int d, fetch_status stat) const {
        assert(parity_bits(p) && d >= 0 && d < NDIRS);
        fs->move_status[(int)p - 1][d] = stat;
    }

    /// check that Field is allocated, and if not do it (if not const)
    /// Must be called BEFORE the var is actually used
    /// "hilapp" will generate these calls as needed!
    void check_alloc() {
        if (!is_allocated())
            allocate();
    }

    /// If Field is const specified, we should not be able to write to it in the first
    /// place
    void check_alloc() const {
        assert(is_allocated());
    }

    // If ALL changes, both parities invalid; if p != ALL, then p and ALL.
    void mark_changed(const Parity p) const {

        for (Direction i = (Direction)0; i < NDIRS; ++i) {
            // check if there's ongoing comms, invalidate it!
            drop_comms(i, opp_parity(p));

            set_move_status(opp_parity(p), i, fetch_status::NOT_DONE);
            if (p != ALL) {
                set_move_status(ALL, i, fetch_status::NOT_DONE);
            } else {
                set_move_status(EVEN, i, fetch_status::NOT_DONE);
                set_move_status(ODD, i, fetch_status::NOT_DONE);
            }
        }
        fs->assigned_to |= parity_bits(p);
    }

    /// Mark the field parity fetched from Direction
    // In case p=ALL we could mark everything fetched, but we'll be conservative here
    // and mark only this parity, because there might be other parities on the fly and
    // corresponding waits should be done,  This should never happen in automatically
    // generated loops. In any case start_fetch, is_fetched, get_move_parity has
    // intelligence to figure out the right thing to do
    //

    void mark_fetched(int dir, const Parity p) const {
        set_move_status(p, dir, fetch_status::DONE);
    }

    // Check if the field has been fetched since the previous communication
    // par = ALL:   ALL or (EVEN+ODD) are OK
    // par != ALL:  ALL or par are OK
    bool is_fetched(int dir, Parity par) const {
        if (par != ALL) {
            return move_status(par, dir) == fetch_status::DONE ||
                   move_status(ALL, dir) == fetch_status::DONE;
        } else {
            return move_status(ALL, dir) == fetch_status::DONE ||
                   (move_status(EVEN, dir) == fetch_status::DONE &&
                    move_status(ODD, dir) == fetch_status::DONE);
        }
    }

    // Mark communication started -- this must be just the one
    // going on with MPI
    void mark_move_started(int dir, Parity p) const {
        set_move_status(p, dir, fetch_status::STARTED);
    }

    /// Check if communication has started.  This is strict, checks exactly this parity
    bool is_move_started(int dir, Parity par) const {
        return move_status(par, dir) == fetch_status::STARTED;
    }

    bool move_not_done(int dir, Parity par) const {
        return move_status(par, dir) == fetch_status::NOT_DONE;
    }

    void set_boundary_condition(Direction dir, BoundaryCondition bc) {

#ifdef SPECIAL_BOUNDARY_CONDITIONS
        // TODO: This works as intended only for periodic/antiperiodic b.c.
        check_alloc();
        fs->boundary_condition[dir] = bc;
        fs->boundary_condition[-dir] = bc;
#if !defined(CUDA) && !defined(HIP)
        fs->neighbours[dir] = lattice->get_neighbour_array(dir, bc);
        fs->neighbours[-dir] = lattice->get_neighbour_array(-dir, bc);
#else
        if (bc == BoundaryCondition::PERIODIC) {
            fs->payload.neighbours[dir] = lattice->backend_lattice->d_neighb[dir];
            fs->payload.neighbours[-dir] = lattice->backend_lattice->d_neighb[-dir];
        } else {
            fs->payload.neighbours[dir] =
                lattice->backend_lattice->d_neighb_special[dir];
            fs->payload.neighbours[-dir] =
                lattice->backend_lattice->d_neighb_special[-dir];
        }
#endif

        // Make sure boundaries get refreshed
        mark_changed(ALL);
#endif
    }

    BoundaryCondition get_boundary_condition(Direction dir) const {
#ifdef SPECIAL_BOUNDARY_CONDITIONS
        return fs->boundary_condition[dir];
#else
        return BoundaryCondition::PERIODIC;
#endif
    }

    void print_boundary_condition() {
        check_alloc();
        output0 << " ( ";
        for (int dir = 0; dir < NDIRS; dir++) {
            output0 << (int)fs->boundary_condition[dir] << " ";
        }
        output0 << ")\n";
    }

    template <typename A>
    void copy_boundary_condition(const Field<A> &rhs) {
        foralldir (dir) {
            set_boundary_condition(dir, rhs.get_boundary_condition(dir));
        }
    }

    // Overloading []
    // declarations -- WILL BE implemented by hilapp, not written here
    // let there be const and non-const protos
    element<T> operator[](const Parity p) const;           // f[EVEN]
    element<T> operator[](const X_index_type) const;       // f[X]
    element<T> operator[](const X_plus_direction p) const; // f[X+dir]
    element<T> operator[](const X_plus_offset p) const;    // f[X+dir1+dir2] and others

    element<T> &operator[](const Parity p);     // f[EVEN]
    element<T> &operator[](const X_index_type); // f[X]

    T &operator[](const CoordinateVector &v);       // f[CoordinateVector]
    T &operator[](const CoordinateVector &v) const; // f[CoordinateVector]

    // TEMPORARY HACK: return ptr to bare array
    inline auto field_buffer() const {
        return fs->payload.get_buffer();
    }

#ifndef VECTORIZED
    /// Get an individual element outside a loop. This is also used as a getter in the
    /// vanilla code.
    inline auto get_value_at(int i) const {
        return fs->get_element(i);
    }
#else
    inline auto get_value_at(int i) const {
        return fs->get_element(i);
    }
    template <typename vecT>
    inline auto get_vector_at(int i) const {
        return fs->template get_vector<vecT>(i);
    }
    inline auto get_value_at_nb_site(Direction d, int i) const {
        return fs->get_element(fs->vector_lattice->site_neighbour(d, i));
    }
#endif

#ifndef VECTORIZED
    /// Set an individual element outside a loop. This is also used as a setter in the
    /// vanilla code.
    template <typename A>
    inline void set_value_at(const A &value, int i) {
        fs->set_element(value, i);
    }

#else
    template <typename vecT>
    inline void set_vector_at(const vecT &value, int i) {
        fs->set_vector(value, i);
    }

    template <typename A>
    inline void set_value_at(const A &value, int i) {
        fs->set_element(value, i);
    }
#endif

    /////////////////////////////////////////////////////////////////
    /// Standard arithmetic ops which fields should implement
    /// Not all are always callable, e.g. division may not be
    /// implemented by all field types
    /////////////////////////////////////////////////////////////////

    // Basic assignment operator
    Field<T> &operator=(const Field<T> &rhs) {
        (*this)[ALL] = rhs[X];
        return *this;
    }

    // More general = - possible only if T = A is OK
    template <typename A, std::enable_if_t<hila::is_assignable<T &, A>::value ||
                                               std::is_convertible<A, T>::value,
                                           int> = 0>
    Field<T> &operator=(const Field<A> &rhs) {
        (*this)[ALL] = rhs[X];
        return *this;
    }

    // Assign from element
    template <typename A, std::enable_if_t<hila::is_assignable<T &, A>::value ||
                                               std::is_convertible<A, T>::value,
                                           int> = 0>
    Field<T> &operator=(const A &d) {
        (*this)[ALL] = d;
        return *this;
    }

    // assignment of 0 - nullptr, zeroes field
    Field<T> &operator=(const std::nullptr_t &z) {
        (*this)[ALL] = 0;
        return *this;
    }

    // Do also move assignment
    Field<T> &operator=(Field<T> &&rhs) {
        if (this != &rhs) {
            free();
            fs = rhs.fs;
            rhs.fs = nullptr;
        }
        return *this;
    }

    // +=, -=  etc operators from compatible types
    template <
        typename A,
        std::enable_if_t<std::is_convertible<hila::type_plus<T, A>, T>::value, int> = 0>
    Field<T> &operator+=(const Field<A> &rhs) {
        (*this)[ALL] += rhs[X];
        return *this;
    }

    template <typename A,
              std::enable_if_t<std::is_convertible<hila::type_minus<T, A>, T>::value,
                               int> = 0>
    Field<T> &operator-=(const Field<A> &rhs) {
        (*this)[ALL] -= rhs[X];
        return *this;
    }

    template <
        typename A,
        std::enable_if_t<std::is_convertible<hila::type_mul<T, A>, T>::value, int> = 0>
    Field<T> &operator*=(const Field<A> &rhs) {
        (*this)[ALL] *= rhs[X];
        return *this;
    }

    template <
        typename A,
        std::enable_if_t<std::is_convertible<hila::type_div<T, A>, T>::value, int> = 0>
    Field<T> &operator/=(const Field<A> &rhs) {
        (*this)[ALL] /= rhs[X];
        return *this;
    }

    template <
        typename A,
        std::enable_if_t<std::is_convertible<hila::type_plus<T, A>, T>::value, int> = 0>
    Field<T> &operator+=(const A &rhs) {
        (*this)[ALL] += rhs;
        return *this;
    }

    template <typename A,
              std::enable_if_t<std::is_convertible<hila::type_minus<T, A>, T>::value,
                               int> = 0>
    Field<T> &operator-=(const A &rhs) {
        (*this)[ALL] -= rhs;
        return *this;
    }

    template <
        typename A,
        std::enable_if_t<std::is_convertible<hila::type_mul<T, A>, T>::value, int> = 0>
    Field<T> &operator*=(const A &rhs) {
        (*this)[ALL] *= rhs;
        return *this;
    }

    template <
        typename A,
        std::enable_if_t<std::is_convertible<hila::type_div<T, A>, T>::value, int> = 0>
    Field<T> &operator/=(const A &rhs) {
        (*this)[ALL] /= rhs;
        return *this;
    }

    // Unary + and -
    Field<T> operator+() const {
        return *this;
    }

    Field<T> operator-() const {
        Field<T> f;
        f[ALL] = -(*this)[X];
        return f;
    }

    hila::number_type<T> squarenorm() const {
        hila::number_type<T> n = 0;
        onsites (ALL) { n += ::squarenorm((*this)[X]); }
        return n;
    }

    ///////////////////////////////////////////////////////////////////////

    // Communication routines
    dir_mask_t start_fetch(Direction d, Parity p = ALL) const;
    void wait_fetch(Direction d, Parity p) const;
    void fetch(Direction d, Parity p = ALL) const;
    void drop_comms(Direction d, Parity p) const;
    void cancel_comm(Direction d, Parity p) const;

    // Declaration of shift methods
    Field<T> &shift(const CoordinateVector &v, Field<T> &r, Parity par) const;
    Field<T> &shift(const CoordinateVector &v, Field<T> &r) const {
        return shift(v, r, ALL);
    }
    Field<T> shift(const CoordinateVector &v, Parity par) const;

    // General getters and setters
    void set_elements(T *elements, const std::vector<CoordinateVector> &coord_list);
    void set_element(const T &element, const CoordinateVector &coord);
    void get_elements(T *elements,
                      const std::vector<CoordinateVector> &coord_list) const;
    T get_element(const CoordinateVector &coord) const;

    template <typename A, std::enable_if_t<std::is_assignable<T &, A>::value, int> = 0>
    inline void set_element_at(const CoordinateVector coord, const A elem) {
        T e;
        e = elem;
        set_element(e, coord);
    }

    inline void set_element_at(const CoordinateVector coord, std::nullptr_t elem) {
        T e;
        e = 0;
        set_element(e, coord);
    }

    // Fourier transform declarations
    void FFT(fft_direction fdir = fft_direction::forward);

    // Writes the Field to disk
    void write_to_stream(std::ofstream &outputfile);
    void write_to_file(const std::string &filename);
    void read_from_stream(std::ifstream &inputfile);
    void read_from_file(const std::string &filename);

    void write_subvolume(std::ofstream &outputfile, const CoordinateVector &cmin,
                         const CoordinateVector &cmax,
                         const std::string &separator = "\n");
    void write_subvolume(const std::string &filenname, const CoordinateVector &cmin,
                         const CoordinateVector &cmax,
                         const std::string &separator = "\n");

    template <typename F>
    void write_slice(F &outputfile, const CoordinateVector &slice,
                     const std::string &separator = "\n");


    // and sum reduction
    T reduce_sum(bool allreduce = true) const;

    /// Declare gpu_reduce here, defined only for GPU targets
    /// For internal use only, preferably
    T gpu_reduce_sum(bool allreduce = true, Parity par = Parity::all,
                     bool do_mpi = true) const;

}; // End of class Field<>

// these operators rely on SFINAE, OK if field_hila::type_plus<A,B> exists i.e. A+B is
// OK
/// operator +
template <typename A, typename B>
auto operator+(Field<A> &lhs, Field<B> &rhs) -> Field<hila::type_plus<A, B>> {
    Field<hila::type_plus<A, B>> tmp;
    tmp[ALL] = lhs[X] + rhs[X];
    return tmp;
}

template <typename A, typename B>
auto operator+(const A &lhs, const Field<B> &rhs) -> Field<hila::type_plus<A, B>> {
    Field<hila::type_plus<A, B>> tmp;
    tmp[ALL] = lhs + rhs[X];
    return tmp;
}

template <typename A, typename B>
auto operator+(const Field<A> &lhs, const B &rhs) -> Field<hila::type_plus<A, B>> {
    Field<hila::type_plus<A, B>> tmp;
    tmp[ALL] = lhs[X] + rhs;
    return tmp;
}

/// operator -
template <typename A, typename B>
auto operator-(const Field<A> &lhs, const Field<B> &rhs)
    -> Field<hila::type_minus<A, B>> {
    Field<hila::type_minus<A, B>> tmp;
    tmp[ALL] = lhs[X] - rhs[X];
    return tmp;
}

template <typename A, typename B>
auto operator-(const A &lhs, const Field<B> &rhs) -> Field<hila::type_minus<A, B>> {
    Field<hila::type_minus<A, B>> tmp;
    tmp[ALL] = lhs - rhs[X];
    return tmp;
}

template <typename A, typename B>
auto operator-(const Field<A> &lhs, const B &rhs) -> Field<hila::type_minus<A, B>> {
    Field<hila::type_minus<A, B>> tmp;
    tmp[ALL] = lhs[X] - rhs;
    return tmp;
}

/// operator *
template <typename A, typename B>
auto operator*(const Field<A> &lhs, const Field<B> &rhs)
    -> Field<hila::type_mul<A, B>> {
    Field<hila::type_mul<A, B>> tmp;
    tmp[ALL] = lhs[X] * rhs[X];
    return tmp;
}

template <typename A, typename B>
auto operator*(const A &lhs, const Field<B> &rhs) -> Field<hila::type_mul<A, B>> {
    Field<hila::type_mul<A, B>> tmp;
    tmp[ALL] = lhs * rhs[X];
    return tmp;
}

template <typename A, typename B>
auto operator*(const Field<A> &lhs, const B &rhs) -> Field<hila::type_mul<A, B>> {
    Field<hila::type_mul<A, B>> tmp;
    tmp[ALL] = lhs[X] * rhs;
    return tmp;
}

/// operator /
template <typename A, typename B>
auto operator/(const Field<A> &lhs, const Field<B> &rhs)
    -> Field<hila::type_div<A, B>> {
    Field<hila::type_div<A, B>> tmp;
    tmp[ALL] = lhs[X] / rhs[X];
    return tmp;
}

template <typename A, typename B>
auto operator/(const A &lhs, const Field<B> &rhs) -> Field<hila::type_div<A, B>> {
    Field<hila::type_div<A, B>> tmp;
    tmp[ALL] = lhs / rhs[X];
    return tmp;
}

template <typename A, typename B>
auto operator/(const Field<A> &lhs, const B &rhs) -> Field<hila::type_div<A, B>> {
    Field<hila::type_div<A, B>> tmp;
    tmp[ALL] = lhs[X] / rhs;
    return tmp;
}


#define NAIVE_SHIFT
#if defined(NAIVE_SHIFT)

/// Definition of shift - this is currently OK only for short moves,
/// very inefficient for longer moves
/// TODO: make more advanced, switching to "global" move for long shifts
/// Returns a reference to parameter "res"

template <typename T>
Field<T> &Field<T>::shift(const CoordinateVector &v, Field<T> &res,
                          const Parity par) const {

    // use this to store remaining moves
    CoordinateVector rem = v;

    // check the parity of the move
    Parity par_s;

    int len = 0;
    foralldir (d)
        len += abs(rem[d]);

    // no move, just copy field
    if (len == 0) {
        res = *this;
        return res;
    }

    // opp_parity(ALL) == ALL
    if (len % 2 == 0)
        par_s = opp_parity(par);
    else
        par_s = par;

    // is this already fetched from one of the dirs in v?
    bool found_dir = false;
    Direction mdir;
    foralldir (d) {
        if (rem[d] > 0 && move_status(par_s, d) != fetch_status::NOT_DONE) {
            mdir = d;
            found_dir = true;
            break;
        } else if (rem[d] < 0 && move_status(par_s, -d) != fetch_status::NOT_DONE) {
            mdir = -d;
            found_dir = true;
            break;
        }
    }

    if (!found_dir) {
        // now did not find a 'ready' dir. Take the 1st available
        foralldir (d) {
            if (rem[d] > 0) {
                mdir = d;
                break;
            } else if (rem[d] < 0) {
                mdir = -d;
                break;
            }
        }
    }

    // Len 1, copy directly
    if (len == 1) {
        res[par_s] = (*this)[X + mdir];
        return res;
    }

    // now longer - need buffer
    Field<T> r1;
    Field<T> *from, *to;

    // this ensures that the final move lands on res
    if (len % 2 == 0) {
        from = &r1;
        to = &res;
    } else {
        from = &res;
        to = &r1;
    }
    // and copy initially to "from"
    (*from)[par_s] = (*this)[X + mdir];

    // and subtract remaining moves from rem
    rem = rem - mdir;
    par_s = opp_parity(par_s);

    foralldir (d) {
        if (rem[d] != 0) {
            mdir = (rem[d] > 0) ? d : -d;

            while (rem[d] != 0) {

                (*to)[par_s] = (*from)[X + mdir];

                par_s = opp_parity(par_s);
                rem = rem - mdir;
                std::swap(to, from);
            }
        }
    }

    return res;
}

template <typename T>
Field<T> Field<T>::shift(const CoordinateVector &v, const Parity par) const {
    Field<T> res;
    shift(v, res, par);
    return res;
}
#elif !defined(USE_MPI)

// this is junk at the moment
template <typename T>
Field<T> &Field<T>::shift(const CoordinateVector &v, Field<T> &res,
                          const Parity par) const {

    onsites (par) { if }
    r2 = *this;
    foralldir (d) {
        if (abs(v[d]) > 0) {
            Direction dir;
            if (v[d] > 0)
                dir = d;
            else
                dir = -d;

            for (int i = 0; i < abs(v[d]); i++) {
                r1[ALL] = r2[X + dir];
                r2 = r1;
            }
        }
    }
    return r2;
}

#endif

#if defined(USE_MPI)

/// start_fetch(): Communicate the field at Parity par from Direction
/// d. Uses accessors to prevent dependency on the layout.
/// return the Direction mask bits where something is happening
template <typename T>
dir_mask_t Field<T>::start_fetch(Direction d, Parity p) const {

    // get the mpi message tag right away, to ensure that we are always synchronized
    // with the mpi calls -- some nodes might not need comms, but the tags must be in
    // sync

    int tag = get_next_msg_tag();

    lattice_struct::nn_comminfo_struct &ci = lattice->nn_comminfo[d];
    lattice_struct::comm_node_struct &from_node = ci.from_node;
    lattice_struct::comm_node_struct &to_node = ci.to_node;

    // check if this is done - either fetched or no comm to be done in the 1st place

    if (is_fetched(d, p)) {
        lattice->n_gather_avoided++;
        return 0; // nothing to wait for
    }

    // No comms to do, nothing to wait for -- we'll use the is_fetched
    // status to keep track of vector boundary shuffle anyway

    if (from_node.rank == hila::myrank() && to_node.rank == hila::myrank()) {
        fs->set_local_boundary_elements(d, p);
        mark_fetched(d, p);
        return 0;
    }

    // if this parity or ALL-type fetch is going on nothing to be done
    if (!move_not_done(d, p) || !move_not_done(d, ALL)) {
        lattice->n_gather_avoided++;
        return get_dir_mask(d); // nothing to do, but still need to wait
    }

    Parity par = p;
    // if p is ALL but ODD or EVEN is going on/done, turn off parity which is not needed
    // corresponding wait must do the same thing
    if (p == ALL) {
        if (!move_not_done(d, EVEN) && !move_not_done(d, ODD)) {
            // even and odd are going on or ready, nothing to be done
            lattice->n_gather_avoided++;
            return get_dir_mask(d);
        }
        if (!move_not_done(d, EVEN))
            par = ODD;
        else if (!move_not_done(d, ODD))
            par = EVEN;
        // if neither is the case par = ALL
    }

    mark_move_started(d, par);

    // Communication hasn't been started yet, do it now

    int par_i = static_cast<int>(par) - 1; // index to dim-3 arrays

    constexpr size_t size = sizeof(T);

    T *receive_buffer;
    T *send_buffer;

    int size_type;
    MPI_Datatype mpi_type = get_MPI_number_type<T>(size_type);

    if (from_node.rank != hila::myrank()) {

        // HANDLE RECEIVES: get node which will send here
        post_receive_timer.start();

        // buffer can be separate or in Field buffer
        receive_buffer = fs->get_receive_buffer(d, par, from_node);

        size_t n = from_node.n_sites(par) * size / size_type;

        if (n >= (1ULL << 31)) {
            hila::output << "Too large MPI message!  Size " << n << '\n';
            hila::terminate(1);
        }

        // c++ version does not return errors
        MPI_Irecv(receive_buffer, n, mpi_type, from_node.rank, tag,
                  lattice->mpi_comm_lat, &fs->receive_request[par_i][d]);

        post_receive_timer.stop();
    }

    if (to_node.rank != hila::myrank()) {
        // HANDLE SENDS: Copy Field elements on the boundary to a send buffer and send
        start_send_timer.start();

        unsigned sites = to_node.n_sites(par);

        if (fs->send_buffer[d] == nullptr)
            fs->send_buffer[d] = fs->payload.allocate_mpi_buffer(to_node.sites);
        send_buffer = fs->send_buffer[d] + to_node.offset(par);

        fs->gather_comm_elements(d, par, send_buffer, to_node);

        size_t n = sites * size / size_type;

        MPI_Isend(send_buffer, n, mpi_type, to_node.rank, tag, lattice->mpi_comm_lat,
                  &fs->send_request[par_i][d]);
        start_send_timer.stop();
    }

    // and do the boundary shuffle here, after MPI has started
    // NOTE: there should be no danger of MPI and shuffle overwriting, MPI writes
    // to halo buffers only if no permutation is needed.  With a permutation MPI
    // uses special receive buffer
    fs->set_local_boundary_elements(d, par);

    return get_dir_mask(d);
}

///  wait_fetch(): Wait for communication at parity par from
///  Direction d completes the communication in the function.
///  If the communication has not started yet, also calls
///  start_fetch()
///
///  NOTE: This will be called even if the field is marked const.
///  Therefore this function is const, even though it does change
///  the internal content of the field, the halo. From the point
///  of view of the user, the value of the field does not change.
template <typename T>
void Field<T>::wait_fetch(Direction d, Parity p) const {

    lattice_struct::nn_comminfo_struct &ci = lattice->nn_comminfo[d];
    lattice_struct::comm_node_struct &from_node = ci.from_node;
    lattice_struct::comm_node_struct &to_node = ci.to_node;

    // check if this is done - either fetched or no comm to be done in the 1st place
    if (is_fetched(d, p))
        return;

    // this is the branch if no comms -- shuffle was done in start_fetch
    if (from_node.rank == hila::myrank() && to_node.rank == hila::myrank())
        return;

    // if (!is_move_started(d,p)) {
    //   output0 << "Wait move error - wait_fetch without corresponding start_fetch\n";
    //   exit(1);
    // }

    // Note: the move can be Parity p OR ALL -- need to wait for it in any case
    // set par to be the "sum" over both parities
    // There never should be ongoing ALL and other parity fetch -- start_fetch takes
    // care

    // check here consistency, this should never happen
    if (p != ALL && is_move_started(d, p) && is_move_started(d, ALL)) {
        exit(1);
    }

    Parity par;
    int n_wait = 1;
    // what par to wait for?
    if (is_move_started(d, p))
        par = p; // standard match
    else if (p != ALL) {
        if (is_move_started(d, ALL))
            par = ALL; // if all is running wait for it
        else {
            exit(1);
        }
    } else {
        // now p == ALL and ALL is not running
        if (is_fetched(d, EVEN) && is_move_started(d, ODD))
            par = ODD;
        else if (is_fetched(d, ODD) && is_move_started(d, EVEN))
            par = EVEN;
        else if (is_move_started(d, EVEN) && is_move_started(d, ODD)) {
            n_wait = 2; // need to wait for both!
            par = ALL;
        } else {
            exit(1);
        }
    }

    if (n_wait == 2)
        par = EVEN; // we'll flip both

    for (int wait_i = 0; wait_i < n_wait; ++wait_i) {

        int par_i = (int)par - 1;

        if (from_node.rank != hila::myrank()) {
            wait_receive_timer.start();

            MPI_Status status;
            MPI_Wait(&fs->receive_request[par_i][d], &status);

            wait_receive_timer.stop();

#ifndef VANILLA
            fs->place_comm_elements(d, par, fs->get_receive_buffer(d, par, from_node),
                                    from_node);
#endif
        }

        // then wait for the sends
        if (to_node.rank != hila::myrank()) {
            wait_send_timer.start();
            MPI_Status status;
            MPI_Wait(&fs->send_request[par_i][d], &status);
            wait_send_timer.stop();
        }

        // Mark the parity fetched from Direction dir
        mark_fetched(d, par);

        // Keep count of communications
        lattice->n_gather_done += 1;

        par = opp_parity(par); // flip if 2 loops
    }
}

///  drop_comms():  if field is changed or deleted,
///  cancel ongoing communications.  This should happen very seldom,
///  only if there are "by-hand" start_fetch operations and these are not needed
template <typename T>
void Field<T>::drop_comms(Direction d, Parity p) const {

    if (is_comm_initialized()) {
        if (is_move_started(d, ALL))
            cancel_comm(d, ALL);
        if (p != ALL) {
            if (is_move_started(d, p))
                cancel_comm(d, p);
        } else {
            if (is_move_started(d, EVEN))
                cancel_comm(d, EVEN);
            if (is_move_started(d, ODD))
                cancel_comm(d, ODD);
        }
    }
}

/// cancel ongoing send and receive

template <typename T>
void Field<T>::cancel_comm(Direction d, Parity p) const {
    if (lattice->nn_comminfo[d].from_node.rank != hila::myrank()) {
        cancel_receive_timer.start();
        MPI_Cancel(&fs->receive_request[(int)p - 1][d]);
        cancel_receive_timer.stop();
    }
    if (lattice->nn_comminfo[d].to_node.rank != hila::myrank()) {
        cancel_send_timer.start();
        MPI_Cancel(&fs->send_request[(int)p - 1][d]);
        cancel_send_timer.stop();
    }
}

#else // No MPI now

///* Trivial implementation when no MPI is used

template <typename T>
dir_mask_t Field<T>::start_fetch(Direction d, Parity p) const {
    // Update local elements in the halo (necessary for vectorized version)
    // We use here simpler tracking than in MPI, may lead to slight extra work
    if (!is_fetched(d, p)) {
        fs->set_local_boundary_elements(d, p);
        mark_fetched(d, p);
    }
    return 0;
}

template <typename T>
void Field<T>::wait_fetch(Direction d, Parity p) const {}

template <typename T>
void Field<T>::drop_comms(Direction d, Parity p) const {}

#endif // MPI

/// And a convenience combi function
template <typename T>
void Field<T>::fetch(Direction d, Parity p) const {
    start_fetch(d, p);
    wait_fetch(d, p);
}

#if defined(USE_MPI)

/// Gather a list of elements to a single node
template <typename T>
void Field<T>::field_struct::gather_elements(T *buffer,
                                             std::vector<CoordinateVector> coord_list,
                                             int root) const {
    std::vector<unsigned> index_list;
    std::vector<unsigned> node_list(lattice->n_nodes());
    std::fill(node_list.begin(), node_list.end(), 0);

    for (CoordinateVector c : coord_list) {
        if (lattice->is_on_mynode(c)) {
            index_list.push_back(lattice->site_index(c));
        }

        node_list[lattice->node_rank(c)]++;
    }

    std::vector<T> send_buffer(index_list.size());
    payload.gather_elements((T *)send_buffer.data(), index_list.data(),
                            send_buffer.size(), lattice);
    if (hila::myrank() != root && node_list[hila::myrank()] > 0) {
        MPI_Send((char *)send_buffer.data(), node_list[hila::myrank()] * sizeof(T),
                 MPI_BYTE, root, hila::myrank(), lattice->mpi_comm_lat);
    }
    if (hila::myrank() == root) {
        for (int n = 0; n < node_list.size(); n++)
            if (node_list[n] > 0) {
                if (n != root) {
                    MPI_Status status;
                    MPI_Recv(buffer, node_list[n] * sizeof(T), MPI_BYTE, n, n,
                             lattice->mpi_comm_lat, &status);
                } else {
                    std::memcpy(buffer, (char *)send_buffer.data(),
                                node_list[n] * sizeof(T));
                }
                buffer += node_list[n];
            }
    }
}

/// Send elements from a single node to a list of coordinates
template <typename T>
void Field<T>::field_struct::send_elements(T *buffer,
                                           std::vector<CoordinateVector> coord_list,
                                           int root) {
    std::vector<unsigned> index_list;
    std::vector<unsigned> node_list(lattice->n_nodes());
    std::fill(node_list.begin(), node_list.end(), 0);

    for (CoordinateVector c : coord_list) {
        if (lattice->is_on_mynode(c)) {
            index_list.push_back(lattice->site_index(c));
        }

        node_list[lattice->node_rank(c)]++;
    }

    std::vector<T> recv_buffer(index_list.size());
    payload.gather_elements((T *)recv_buffer.data(), index_list.data(),
                            recv_buffer.size(), lattice);
    if (hila::myrank() != root && node_list[hila::myrank()] > 0) {
        MPI_Status status;
        MPI_Recv((char *)recv_buffer.data(), node_list[hila::myrank()] * sizeof(T),
                 MPI_BYTE, root, hila::myrank(), lattice->mpi_comm_lat, &status);
    }
    if (hila::myrank() == root) {
        for (int n = 0; n < node_list.size(); n++)
            if (node_list[n] > 0) {
                if (n != root) {
                    MPI_Send(buffer, node_list[n] * sizeof(T), MPI_BYTE, n, n,
                             lattice->mpi_comm_lat);
                } else {
                    std::memcpy((char *)recv_buffer.data(), buffer,
                                node_list[n] * sizeof(T));
                }
                buffer += node_list[n];
            }
    }
    payload.place_elements((T *)recv_buffer.data(), index_list.data(),
                           recv_buffer.size(), lattice);
}

#else // Now not USE_MPI

/// Gather a list of elements to a single node
template <typename T>
void Field<T>::field_struct::gather_elements(T *buffer,
                                             std::vector<CoordinateVector> coord_list,
                                             int root) const {
    std::vector<unsigned> index_list;
    for (CoordinateVector c : coord_list) {
        index_list.push_back(lattice->site_index(c));
    }

    payload.gather_elements(buffer, index_list.data(), index_list.size(), lattice);
}

/// Send elements from a single node to a list of coordinates
template <typename T>
void Field<T>::field_struct::send_elements(T *buffer,
                                           std::vector<CoordinateVector> coord_list,
                                           int root) {
    std::vector<unsigned> index_list;
    for (CoordinateVector c : coord_list) {
        index_list.push_back(lattice->site_index(c));
    }

    payload.place_elements(buffer, index_list.data(), index_list.size(), lattice);
}

#endif

/// Functions for manipulating individual elements in an array

/// Set an element. Assuming that each node calls this with the same value, it is
/// sufficient to set the elements locally
template <typename T>
void Field<T>::set_elements(T *elements,
                            const std::vector<CoordinateVector> &coord_list) {
    std::vector<unsigned> my_indexes;
    std::vector<unsigned> my_elements;
    for (int i = 0; i < coord_list.size(); i++) {
        CoordinateVector c = coord_list[i];
        if (lattice->is_on_mynode(c)) {
            my_indexes.push_back(lattice->site_index(c));
            my_elements.push_back(elements[i]);
        }
    }
    fs->payload.place_elements(my_elements.data(), my_indexes.data(), my_indexes.size(),
                               lattice);
    mark_changed(ALL);
}

// Set a single element. Assuming that each node calls this with the same value, it is
/// sufficient to set the element locally
template <typename T>
void Field<T>::set_element(const T &element, const CoordinateVector &coord) {
    if (lattice->is_on_mynode(coord)) {
        set_value_at(element, lattice->site_index(coord));
    }
    mark_changed(ALL);
}

/// Get an element and return it on all nodes
#if defined(USE_MPI)
/// This is not local, the element needs to be communicated to all nodes
template <typename T>
T Field<T>::get_element(const CoordinateVector &coord) const {
    T element;

    int owner = lattice->node_rank(coord);

    if (hila::myrank() == owner) {
        element = get_value_at(lattice->site_index(coord));
    }

    MPI_Bcast(&element, sizeof(T), MPI_BYTE, owner, lattice->mpi_comm_lat);
    return element;
}

/// Get a list of elements and store them into an array on all nodes
template <typename T>
void Field<T>::get_elements(T *elements,
                            const std::vector<CoordinateVector> &coord_list) const {
    struct node_site_list_struct {
        std::vector<int> indexes;
        std::vector<CoordinateVector> coords;
    };

    std::vector<node_site_list_struct> nodelist(lattice->n_nodes());
    // Reorganize the list according to nodes
    for (int i = 0; i < coord_list.size(); i++) {
        CoordinateVector c = coord_list[i];
        int node = lattice->node_rank(c);
        nodelist[node].indexes.push_back(i);
        nodelist[node].coords.push_back(c);
    }

    // Fetch on each node found and communicate
    for (int n = 0; n < nodelist.size(); n++) {
        node_site_list_struct node = nodelist[n];
        if (node.indexes.size() > 0) {
            T *element_buffer = (T *)memalloc(sizeof(T) * node.indexes.size());
            fs->payload.gather_elements(element_buffer, node.coords);

            MPI_Bcast(&element_buffer, sizeof(T), MPI_BYTE, n, lattice->mpi_comm_lat);

            // place in the array in original order
            for (int i = 0; i < node.indexes.size(); i++) {
                elements[i] = element_buffer[node.indexes[i]];
            }

            std::free(element_buffer);
        }
    }
}

#else
/// Without MPI, we just need to call get
template <typename T>
T Field<T>::get_element(const CoordinateVector &coord) const {
    return get_value_at(lattice->site_index(coord));
}

/// Without MPI, we just need to call get
template <typename T>
void Field<T>::get_elements(T *elements,
                            const std::vector<CoordinateVector> &coord_list) const {
    for (int i = 0; i < coord_list.size(); i++) {
        elements[i] = (*this)[coord_list[i]];
    }
}
#endif


#ifdef HILAPP
// Removed by hilapp
#endif

#endif // FIELD_H
// end include "plumbing/field.h"---------------------------------

#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/field_io.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/reduction.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/vectorreduction.h"

#if defined(CUDA) || defined(HIP)
#include "plumbing/backend_cuda/gpu_reduction.h"
#endif

#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/vector_field.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/fft.h"
#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/input.h"

#if defined(OPENMP)
#include <omp.h>
#endif

#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/random.h"

#endif// end include "hila.h"---------------------------------

#include "/cluster/home/alopez07/HILA/hila/libraries/plumbing/random.h"

/////////////////////////////////////////////////////////////////////////

#include <random>

//#ifndef OPENMP

// static variable which holds the random state
// Use 64-bit mersenne twister
static std::mt19937_64 mersenne_twister_gen;

// random numbers are in interval [0,1)
static std::uniform_real_distribution<double> real_rnd_dist(0.0,1.0);

//#endif


// In GPU code hila::random() defined in hila_gpu.cpp
#if !defined(CUDA) && !defined(HIP)

double hila::random() {
    return real_rnd_dist( mersenne_twister_gen );
}

#endif

// Generate random number in non-kernel (non-loop) code.  Not meant to
// be used in "user code"
double hila::host_random() {
    return real_rnd_dist( mersenne_twister_gen );
}

/////////////////////////////////////////////////////////////////////////

/// Seed random number generators
/// Seed is shuffled so that different nodes
/// get different rng seeds.  If seed == 0,
/// generate seed using the time() -function.

void hila::seed_random(uint64_t seed) {

#ifndef SITERAND

    uint64_t n = hila::myrank();
    if (sublattices.number > 1)
        n += sublattices.mylattice * hila::number_of_nodes();

    if (seed == 0) {
        // get seed from time
        if (hila::myrank() == 0) {
            struct timespec tp;

            clock_gettime(CLOCK_MONOTONIC, &tp);
            seed = tp.tv_sec;
            seed = (seed << 30) ^ tp.tv_nsec;
            output0 << "Random seed from time: " << seed << '\n';
        }
        broadcast(seed);
    }
    // do node shuffling for seed
    // do it in a manner makes it difficult to give the same seed by mistake
    // and also avoids giving the same seed for 2 nodes
    // n=0 remains unchanged

    seed = seed ^ (n ^ ((7*n) << 25));

    output0 << "Using node random numbers, seed for node 0: " << seed << '\n';

// #if !defined(OPENMP)
    mersenne_twister_gen.seed( seed );
    // warm it up
    for (int i = 0; i < 9000; i++)
        mersenne_twister_gen();
// #endif


#if defined(CUDA) || defined(HIP)
    // we can use the same seed, the generator is different
    hila::seed_device_rng(seed);
#endif

    // taus_initialize();

#else

    // TODO: clean SITERAND!
    // Now SITERAND is defined
    // This is usually used only for occasional benchmarking, where identical output
    // independent of the node number is desired

    output0 << "*** SITERAND is in use!\n";

    random_seed_arr =
        (unsigned short(*)[3])memalloc(3 * node.sites * sizeof(unsigned short));
    forallsites(i) {
        random_seed_arr[i][0] = (unsigned short)(seed + site[i].index);
        random_seed_arr[i][1] = (unsigned short)(seed + 2 * site[i].index);
        random_seed_arr[i][2] = (unsigned short)(seed + 3 * site[i].index);
    }

    random_seed_ptr = random_seed_arr[0];

#endif
}




///////////////////////////////////////////////////////////////////
/// gaussian rng generation routines
/// By default these give random numbers with variance 1.0, i.e.
/// probability distribution is
///           exp( -x*x/2 ), so < x^2 > = 1
/// If you want random numbers with variance sigma, multiply the
/// result by  sqrt(sigma):
///       sqrt(sigma) * gaussrand();
////////////////////////////////////////////////////////////////////

#define VARIANCE 1.0

double hila::gaussrand2(double &out2) {

    double phi, urnd, r;
    phi = 2.0 * M_PI * hila::random();

    // this should not really trigger
    do {
        urnd = 1.0 - hila::random();
    } while (urnd == 0.0);

    r = sqrt(-::log(urnd) * (2.0 * VARIANCE));
    out2 = r * cos(phi);
    return r * sin(phi);
}

#if !defined(CUDA) && !defined(HIP)

double hila::gaussrand() {
    static double second;
    static bool draw_new = true;
    if (draw_new) {
        draw_new = false;
        return hila::gaussrand2(second);
    }
    draw_new = true;
    return second;
}

#else

// Cuda and other stuff which does not accept
// static variables - just throw away another gaussian number.

//#pragma hila loop function contains rng
double hila::gaussrand() {
    double second;
    return hila::gaussrand2(second);
}

#endif
